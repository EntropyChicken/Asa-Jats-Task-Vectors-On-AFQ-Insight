{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n",
    "from afqinsight import AFQDataset\n",
    "from afqinsight.nn.utils import prep_pytorch_data\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions.normal import Normal\n",
    "from sklearn.decomposition import PCA\n",
    "import afqinsight.augmentation as aug\n",
    "from afqinsight.nn.pt_models import Conv1DAutoencoder\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Adjust path as needed\n",
    "sys.path.insert(1, '/mmfs1/gscratch/nrdg/samchou/AFQ-Insight-Autoencoder-Experiments/Experiment_Utils')\n",
    "# sys.path.insert(1, '/Users/samchou/AFQ-Insight-Autoencoder-Experiments/AFQ-Insight-Autoencoder-Experiments/Experiment_Utils')\n",
    "# Import necessary functions, including the new one\n",
    "from utils import select_device, kl_divergence_loss,prep_fa_flattned_data, prep_fa_flattened_remapped_data, train_vae_age_site_staged\n",
    "from models import Conv1DVariationalAutoencoder_fa, AgePredictorCNN, SitePredictorCNN, CombinedVAE_Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = select_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AFQDataset.from_study('hbn')\n",
    "torch_dataset, train_loader, test_loader, val_loader = prep_pytorch_data(dataset,batch_size=128)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_idx = dataset.target_cols.index('scan_site_id')\n",
    "age_idx = dataset.target_cols.index('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dataset, all_tracts_train_loader, all_tracts_test_loader, all_tracts_val_loader = prep_fa_flattned_data(dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing initial PyTorch data loaders...\")\n",
    "try:\n",
    "    # Assuming prep_pytorch_data returns torch_dataset, train_loader, test_loader, val_loader\n",
    "    # If it returns datasets, create loaders here.\n",
    "    # Adapt this call based on the actual signature and return values of your prep_pytorch_data\n",
    "    prep_output = prep_fa_flattened_remapped_data(dataset, batch_size=128)\n",
    "    if len(prep_output) == 4:\n",
    "        _, train_loader_raw, test_loader_raw, val_loader_raw = prep_output\n",
    "    else:\n",
    "        raise ValueError(f\"Expected 4 return values from prep_pytorch_data, got {len(prep_output)}\")\n",
    "\n",
    "    print(\"Initial data loaders prepared.\")\n",
    "except Exception as e:\n",
    "     print(f\"Error calling prep_pytorch_data: {e}\")\n",
    "     print(\"Ensure the function exists and returns DataLoaders or required components.\")\n",
    "     sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'x_batch' in locals() and x_batch is not None:\n",
    "    input_channels = x_batch.shape[1]\n",
    "    sequence_length = x_batch.shape[2]\n",
    "    print(f\"Detected input shape: channels={input_channels}, sequence_length={sequence_length}\")\n",
    "else:\n",
    "    print(\"Warning: Could not get sample batch to determine input shape.\")\n",
    "    # Set defaults or exit if necessary\n",
    "    input_channels = 1 # Set manually if needed\n",
    "    sequence_length = 50 # Set manually if needed (MUST MATCH VAE DECODER OUTPUT)\n",
    "    print(f\"Using default/manual input shape: channels={input_channels}, sequence_length={sequence_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... existing code ...\n",
    "\n",
    "# ================================================================================\n",
    "# STAGED TRAINING EXPERIMENT\n",
    "# ================================================================================\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"RUNNING STAGED TRAINING EXPERIMENT\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Set parameters for the staged experiment\n",
    "latent_dim = 64  # Choose the larger latent dim\n",
    "dropout = 0.0  # VAE dropout\n",
    "age_dropout = 0.0\n",
    "site_dropout = 0.0\n",
    "w_recon = 1.0\n",
    "w_kl = 0.1\n",
    "w_age = 5.0  # Higher weight for age prediction\n",
    "w_site = 5.0  # Higher weight for site adversarial training\n",
    "\n",
    "# Create models\n",
    "vae = Conv1DVariationalAutoencoder_fa(latent_dims=latent_dim, dropout=dropout)\n",
    "age_predictor = AgePredictorCNN(input_channels=input_channels, \n",
    "                              sequence_length=sequence_length, \n",
    "                              dropout=age_dropout)\n",
    "site_predictor = SitePredictorCNN(num_sites=4, \n",
    "                                input_channels=input_channels, \n",
    "                                sequence_length=sequence_length, \n",
    "                                dropout=site_dropout)\n",
    "\n",
    "# Define directory for saving staged models\n",
    "staged_save_directory = \"staged_experiment_results\"\n",
    "os.makedirs(staged_save_directory, exist_ok=True)\n",
    "print(f\"Staged experiment results will be saved in: {staged_save_directory}\")\n",
    "\n",
    "\n",
    "    \n",
    "staged_results = train_vae_age_site_staged(\n",
    "    vae_model=vae,\n",
    "    age_predictor=age_predictor,\n",
    "    site_predictor=site_predictor,\n",
    "    train_data=train_loader_raw,\n",
    "    val_data=val_loader_raw,\n",
    "    epochs_stage1=500,  # For individual training\n",
    "    epochs_stage2=1000,  # For adversarial training\n",
    "    lr=0.001,\n",
    "    device=device,\n",
    "    max_grad_norm=1.0,\n",
    "    w_recon=w_recon,\n",
    "    w_kl=w_kl,\n",
    "    w_age=w_age,\n",
    "    w_site=w_site,\n",
    "    kl_annealing_start_epoch=200,\n",
    "    kl_annealing_duration=200,\n",
    "    kl_annealing_start=0.001,\n",
    "    grl_alpha_start=0.0,\n",
    "    grl_alpha_end=2.5,\n",
    "    grl_alpha_epochs=150,\n",
    "    save_dir=staged_save_directory,\n",
    "    val_metric_to_monitor=\"val_age_mae\"\n",
    ")\n",
    "\n",
    "def process_metrics(metrics_dict, keys_to_convert):\n",
    "    processed_results = {}\n",
    "    for key in keys_to_convert:\n",
    "        metric_list = metrics_dict.get(key, [])\n",
    "        new_list = []\n",
    "        if isinstance(metric_list, (list, tuple)):\n",
    "            for val in metric_list:\n",
    "                if isinstance(val, torch.Tensor):\n",
    "                    new_list.append(float(val.cpu().item()))\n",
    "                elif isinstance(val, (int, float, np.number)):\n",
    "                    new_list.append(float(val))\n",
    "                else:\n",
    "                    new_list.append(float('nan'))\n",
    "        processed_results[key] = new_list\n",
    "    return processed_results\n",
    "\n",
    "# Process and save VAE Stage 1 results\n",
    "if staged_results and \"vae\" in staged_results:\n",
    "    vae_results = staged_results[\"vae\"]\n",
    "    \n",
    "    # Keys to convert for VAE metrics\n",
    "    vae_keys = [\n",
    "        \"train_loss_epoch\", \"val_loss_epoch\",\n",
    "        \"train_recon_loss_epoch\", \"val_recon_loss_epoch\",\n",
    "        \"train_kl_loss_epoch\", \"val_kl_loss_epoch\",\n",
    "        \"current_beta_epoch\", \"current_lr_epoch\"\n",
    "    ]\n",
    "    \n",
    "    vae_processed = process_metrics(vae_results, vae_keys)\n",
    "    \n",
    "    # Create DataFrame for VAE metrics\n",
    "    vae_epochs = len(vae_processed.get(\"train_loss_epoch\", []))\n",
    "    if vae_epochs > 0:\n",
    "        vae_df_data = {\"epoch\": range(1, vae_epochs + 1)}\n",
    "        for k in vae_keys:\n",
    "            col_name = k.replace('_epoch', '')\n",
    "            vae_df_data[col_name] = vae_processed.get(k, [float('nan')] * vae_epochs)\n",
    "        \n",
    "        vae_df = pd.DataFrame(vae_df_data)\n",
    "        vae_metrics_file = os.path.join(staged_save_directory, \"vae_metrics.csv\")\n",
    "        vae_df.to_csv(vae_metrics_file, index=False)\n",
    "        print(f\"Saved VAE training metrics to {vae_metrics_file}\")\n",
    "    \n",
    "\n",
    "# Process and save Age Predictor Stage 1 results\n",
    "if staged_results and \"age_predictor\" in staged_results:\n",
    "    age_results = staged_results[\"age_predictor\"]\n",
    "    \n",
    "    # Keys to convert for Age Predictor metrics\n",
    "    age_keys = [\"train_loss_epoch\", \"val_loss_epoch\", \"current_lr_epoch\"]\n",
    "    \n",
    "    age_processed = process_metrics(age_results, age_keys)\n",
    "    \n",
    "    # Create DataFrame for Age Predictor metrics\n",
    "    age_epochs = len(age_processed.get(\"train_loss_epoch\", []))\n",
    "    if age_epochs > 0:\n",
    "        age_df_data = {\"epoch\": range(1, age_epochs + 1)}\n",
    "        for k in age_keys:\n",
    "            col_name = k.replace('_epoch', '')\n",
    "            age_df_data[col_name] = age_processed.get(k, [float('nan')] * age_epochs)\n",
    "        \n",
    "        age_df = pd.DataFrame(age_df_data)\n",
    "        age_metrics_file = os.path.join(staged_save_directory, \"age_predictor_metrics.csv\")\n",
    "        age_df.to_csv(age_metrics_file, index=False)\n",
    "        print(f\"Saved Age Predictor training metrics to {age_metrics_file}\")\n",
    "\n",
    "# Process and save Site Predictor Stage 1 results\n",
    "if staged_results and \"site_predictor\" in staged_results:\n",
    "    site_results = staged_results[\"site_predictor\"]\n",
    "    \n",
    "    # Keys to convert for Site Predictor metrics\n",
    "    site_keys = [\"train_loss_epoch\", \"val_loss_epoch\", \"train_acc_epoch\", \"val_acc_epoch\", \"current_lr_epoch\"]\n",
    "    \n",
    "    site_processed = process_metrics(site_results, site_keys)\n",
    "    \n",
    "    # Create DataFrame for Site Predictor metrics\n",
    "    site_epochs = len(site_processed.get(\"train_loss_epoch\", []))\n",
    "    if site_epochs > 0:\n",
    "        site_df_data = {\"epoch\": range(1, site_epochs + 1)}\n",
    "        for k in site_keys:\n",
    "            col_name = k.replace('_epoch', '')\n",
    "            site_df_data[col_name] = site_processed.get(k, [float('nan')] * site_epochs)\n",
    "        \n",
    "        site_df = pd.DataFrame(site_df_data)\n",
    "        site_metrics_file = os.path.join(staged_save_directory, \"site_predictor_metrics.csv\")\n",
    "        site_df.to_csv(site_metrics_file, index=False)\n",
    "        print(f\"Saved Site Predictor training metrics to {site_metrics_file}\")\n",
    "\n",
    "# Process and save combined stage results\n",
    "if staged_results and \"combined\" in staged_results:\n",
    "    combined_results = staged_results[\"combined\"]\n",
    "    \n",
    "    # Convert metrics to CPU floats\n",
    "    keys_to_convert = [\n",
    "        \"train_loss_epoch\", \"val_loss_epoch\", \n",
    "        \"train_recon_loss_epoch\", \"val_recon_loss_epoch\",\n",
    "        \"train_kl_loss_epoch\", \"val_kl_loss_epoch\", \n",
    "        \"train_age_loss_epoch\", \"val_age_loss_epoch\",\n",
    "        \"train_site_loss_epoch\", \"val_site_loss_epoch\", \n",
    "        \"train_age_mae_epoch\", \"val_age_mae_epoch\",\n",
    "        \"train_site_acc_epoch\", \"val_site_acc_epoch\", \n",
    "        \"current_beta_epoch\", \"current_grl_alpha_epoch\",\n",
    "        \"current_lr_epoch\"\n",
    "    ]\n",
    "    \n",
    "    processed_results = process_metrics(combined_results, keys_to_convert)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    num_epochs = len(processed_results.get(\"train_loss_epoch\", []))\n",
    "    if num_epochs > 0:\n",
    "        df_data = {\"epoch\": range(1, num_epochs + 1)}\n",
    "        for k in keys_to_convert:\n",
    "            col_name = k.replace('_epoch', '')\n",
    "            metric_data = processed_results.get(k, [])\n",
    "            if len(metric_data) != num_epochs:\n",
    "                metric_data = [float('nan')] * num_epochs\n",
    "            df_data[col_name] = metric_data\n",
    "        \n",
    "        df_epochs = pd.DataFrame(df_data)\n",
    "        metrics_file = os.path.join(staged_save_directory, \"staged_combined_metrics.csv\")\n",
    "        df_epochs.to_csv(metrics_file, index=False)\n",
    "        \n",
    "        # Save summary metrics\n",
    "        best_mae_key = \"best_val_age_mae\"\n",
    "        best_mae = combined_results.get(best_mae_key, float('nan'))\n",
    "        if isinstance(best_mae, torch.Tensor):\n",
    "            best_mae = float(best_mae.cpu().item())\n",
    "        \n",
    "        df_summary = pd.DataFrame([{\n",
    "            best_mae_key: best_mae,\n",
    "            \"best_epoch\": combined_results.get(\"best_epoch\", float('nan')),\n",
    "            \"model_path\": combined_results.get(\"model_path\", \"N/A\")\n",
    "        }])\n",
    "        summary_file = os.path.join(staged_save_directory, \"staged_combined_summary.csv\")\n",
    "        df_summary.to_csv(summary_file, index=False)\n",
    "        \n",
    "        print(f\"Saved staged training metrics to {metrics_file}\")\n",
    "        print(f\"Saved staged training summary to {summary_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Staged training experiment complete!\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "print(f\"\\n !!! Staged training experiment failed: {e} !!!\\n\")\n",
    "import traceback\n",
    "traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
