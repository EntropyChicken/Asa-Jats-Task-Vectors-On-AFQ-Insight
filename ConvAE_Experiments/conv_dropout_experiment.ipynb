{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils\n",
    "import torch.distributions\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 200\n",
    "from afqinsight import AFQDataset\n",
    "from afqinsight.nn.utils import prep_pytorch_data\n",
    "from afqinsight.nn.utils import prep_fa_dataset\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions.normal import Normal\n",
    "from sklearn.decomposition import PCA\n",
    "import afqinsight.augmentation as aug\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /Users/samchou/.cache/afq-insight/hbn/subjects.tsv exists.\n",
      "File /Users/samchou/.cache/afq-insight/hbn/nodes.csv exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samchou/miniconda3/envs/afq_new/lib/python3.11/site-packages/afqinsight/transform.py:144: FutureWarning: The previous implementation of stack is deprecated and will be removed in a future version of pandas. See the What's New notes for pandas 2.1.0 for details. Specify future_stack=True to adopt the new implementation and silence this warning.\n",
      "  features = interpolated.stack([\"subjectID\", \"tractID\", \"metric\"]).unstack(\n"
     ]
    }
   ],
   "source": [
    "dataset = AFQDataset.from_study('hbn')\n",
    "torch_dataset, train_loader, test_loader, val_loader = prep_pytorch_data(dataset,batch_size=64)  \n",
    "gt_shape = torch_dataset[0][1].size()[0]\n",
    "sequence_length = torch_dataset[0][0].size()[0]  # 48\n",
    "in_channels = torch_dataset[0][0].size()[1]  # 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1DEncoder_one_tract(nn.Module):\n",
    "    def __init__(self, latent_dims=20, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=5, stride=2, padding=2)  \n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=4, stride=2, padding=2)  \n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=2)\n",
    "        self.conv4 = nn.Conv1d(64, latent_dims, kernel_size=5, stride=2, padding=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.conv1(x)) \n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout(x)   \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv4(x)\n",
    "        return x\n",
    "    \n",
    "class Conv1DDecoder_one_tract(nn.Module):\n",
    "    def __init__(self, latent_dims=20):\n",
    "        super().__init__()\n",
    "\n",
    "        self.deconv1 = nn.ConvTranspose1d(latent_dims, 64, kernel_size=5, stride=2, padding=2, output_padding=0)  \n",
    "        self.deconv2 = nn.ConvTranspose1d(64, 32, kernel_size=5, stride=2, padding=2, output_padding=0)  \n",
    "        self.deconv3 = nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=2, output_padding=2)\n",
    "        self.deconv4 = nn.ConvTranspose1d(16, 1, kernel_size=5, stride=2, padding=2, output_padding=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = F.relu(self.deconv1(x)) \n",
    "        x = F.relu(self.deconv2(x))   \n",
    "        x = F.relu(self.deconv3(x))\n",
    "        x = self.deconv4(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "        # return x.view(batch_size, -1)\n",
    "\n",
    "class Conv1DAutoencoder_one_tract(nn.Module): \n",
    "    def __init__(self, latent_dims=20, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.encoder = Conv1DEncoder_one_tract(latent_dims, dropout=dropout)\n",
    "        self.decoder = Conv1DDecoder_one_tract(latent_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_prime = self.decoder(z)\n",
    "        return x_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1DAutoencoder_one_tract(nn.Module): \n",
    "    def __init__(self, latent_dims=20, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.encoder = Conv1DEncoder_one_tract(latent_dims, dropout=dropout)\n",
    "        self.decoder = Conv1DDecoder_one_tract(latent_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_prime = self.decoder(z)\n",
    "        return x_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(x, x_hat, kl_div=0.0, reduction=\"sum\"):\n",
    "    \"\"\"\n",
    "    Compute the reconstruction loss (MSE) and optionally add a KL term.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : torch.Tensor\n",
    "        Original input tensor.\n",
    "    x_hat : torch.Tensor\n",
    "        Reconstructed output from the autoencoder.\n",
    "    kl_div : float\n",
    "        KL divergence term (VAE only). Defaults to 0 for a standard autoencoder.\n",
    "    reduction : str\n",
    "        Reduction mode: \"sum\" or \"mean\". Defaults to \"sum\".\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        Scalar loss value.\n",
    "    \"\"\"\n",
    "    # e.g. use MSE loss\n",
    "    recon_loss = F.mse_loss(x_hat, x, reduction=reduction)\n",
    "    l1_loss = F.l1_loss(x_hat, x, reduction=reduction)\n",
    "    total_loss = recon_loss + kl_div\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_first_tract_dropout_experiment(\n",
    "    self, train_data, val_data, epochs=20, lr=0.001, num_selected_tracts=5, sigma=0.03\n",
    "):\n",
    "    opt = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, 'min', patience=5, factor=0.5)\n",
    "    train_rmse_per_epoch = []\n",
    "    val_rmse_per_epoch = []\n",
    "    best_val_loss = float('inf')  # Track the best (lowest) validation RMSE overall\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        self.train()\n",
    "        running_loss = 0\n",
    "        running_rmse = 0\n",
    "        items = 0\n",
    "\n",
    "        for x, _ in train_data:  # x shape: (batch_size, 48, 100)\n",
    "            batch_size = x.size(0)\n",
    "            # For simplicity, using only the first tract in training\n",
    "\n",
    "            tract_data = x[:, 0:1, :] #keeps it (batch_size, 1, 100)    \n",
    "\n",
    "            # Apply jitter augmentation\n",
    "            tract_data = tract_data.to(torch.float32).numpy()\n",
    "            tract_data = aug.jitter(tract_data, sigma=sigma)\n",
    "            tract_data = torch.tensor(tract_data, dtype=torch.float32).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            opt.zero_grad()\n",
    "            x_hat = self(tract_data)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = reconstruction_loss(tract_data, x_hat, kl_div=0, reduction=\"sum\")\n",
    "            # Compute RMSE for the batch\n",
    "            batch_rmse = torch.sqrt(F.mse_loss(tract_data, x_hat, reduction=\"mean\"))\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            items += tract_data.size(0)\n",
    "            running_loss += loss.item()\n",
    "            running_rmse += batch_rmse.item() * tract_data.size(0)  # Weighted sum\n",
    "\n",
    "        scheduler.step(running_loss / items)\n",
    "        avg_train_rmse = running_rmse / items\n",
    "        train_rmse_per_epoch.append(avg_train_rmse)\n",
    "\n",
    "        # Validation\n",
    "        self.eval()\n",
    "        val_rmse = 0\n",
    "        val_items = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, _ in val_data:\n",
    "                batch_size = x.size(0)\n",
    "                # For simplicity, using the first tract in validation as well\n",
    "                tract_data = x[:, 0:1, :]\n",
    "\n",
    "                # Apply jitter augmentation\n",
    "                \n",
    "                tract_data = tract_data.to(torch.float32).numpy()\n",
    "                tract_data = aug.jitter(tract_data, sigma=sigma)\n",
    "                tract_data = torch.tensor(tract_data, dtype=torch.float32).to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                x_hat = self(tract_data)\n",
    "\n",
    "                # Compute RMSE for the batch\n",
    "                batch_val_rmse = torch.sqrt(F.mse_loss(tract_data, x_hat, reduction=\"mean\"))\n",
    "\n",
    "                val_items += tract_data.size(0)\n",
    "                val_rmse += batch_val_rmse.item() * tract_data.size(0)\n",
    "\n",
    "        avg_val_rmse = val_rmse / val_items\n",
    "        val_rmse_per_epoch.append(avg_val_rmse)\n",
    "\n",
    "        # Update best validation loss if improved\n",
    "        if avg_val_rmse < best_val_loss:\n",
    "            best_val_loss = avg_val_rmse\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}, Train RMSE: {avg_train_rmse:.4f}, Val RMSE: {avg_val_rmse:.4f}\"\n",
    "        )\n",
    "\n",
    "    return train_rmse_per_epoch, val_rmse_per_epoch, best_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with dropout = 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'reconstruction_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/samchou/AFQ-Insight-Autoencoder-Experiments/AFQ-Insight-Autoencoder-Experiments/ConvAE_Experiments/conv_dropout_experiment.ipynb Cell 7\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samchou/AFQ-Insight-Autoencoder-Experiments/AFQ-Insight-Autoencoder-Experiments/ConvAE_Experiments/conv_dropout_experiment.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTraining with dropout = \u001b[39m\u001b[39m{\u001b[39;00mdropout\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samchou/AFQ-Insight-Autoencoder-Experiments/AFQ-Insight-Autoencoder-Experiments/ConvAE_Experiments/conv_dropout_experiment.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m vae_one_tract \u001b[39m=\u001b[39m Conv1DAutoencoder_one_tract(latent_dims\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, dropout\u001b[39m=\u001b[39mdropout)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/samchou/AFQ-Insight-Autoencoder-Experiments/AFQ-Insight-Autoencoder-Experiments/ConvAE_Experiments/conv_dropout_experiment.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m train_rmse, val_rmse, best_val_loss \u001b[39m=\u001b[39m train_first_tract_dropout_experiment(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samchou/AFQ-Insight-Autoencoder-Experiments/AFQ-Insight-Autoencoder-Experiments/ConvAE_Experiments/conv_dropout_experiment.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     vae_one_tract, train_loader, val_loader, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m, num_selected_tracts\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samchou/AFQ-Insight-Autoencoder-Experiments/AFQ-Insight-Autoencoder-Experiments/ConvAE_Experiments/conv_dropout_experiment.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samchou/AFQ-Insight-Autoencoder-Experiments/AFQ-Insight-Autoencoder-Experiments/ConvAE_Experiments/conv_dropout_experiment.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m train_rmse_results[dropout] \u001b[39m=\u001b[39m train_rmse\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samchou/AFQ-Insight-Autoencoder-Experiments/AFQ-Insight-Autoencoder-Experiments/ConvAE_Experiments/conv_dropout_experiment.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m val_rmse_results[dropout] \u001b[39m=\u001b[39m val_rmse\n",
      "\u001b[1;32m/Users/samchou/AFQ-Insight-Autoencoder-Experiments/AFQ-Insight-Autoencoder-Experiments/ConvAE_Experiments/conv_dropout_experiment.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samchou/AFQ-Insight-Autoencoder-Experiments/AFQ-Insight-Autoencoder-Experiments/ConvAE_Experiments/conv_dropout_experiment.ipynb#W3sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m x_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(tract_data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samchou/AFQ-Insight-Autoencoder-Experiments/AFQ-Insight-Autoencoder-Experiments/ConvAE_Experiments/conv_dropout_experiment.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Calculate loss\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/samchou/AFQ-Insight-Autoencoder-Experiments/AFQ-Insight-Autoencoder-Experiments/ConvAE_Experiments/conv_dropout_experiment.ipynb#W3sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m loss \u001b[39m=\u001b[39m reconstruction_loss(tract_data, x_hat, kl_div\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samchou/AFQ-Insight-Autoencoder-Experiments/AFQ-Insight-Autoencoder-Experiments/ConvAE_Experiments/conv_dropout_experiment.ipynb#W3sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Compute RMSE for the batch\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/samchou/AFQ-Insight-Autoencoder-Experiments/AFQ-Insight-Autoencoder-Experiments/ConvAE_Experiments/conv_dropout_experiment.ipynb#W3sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m batch_rmse \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqrt(F\u001b[39m.\u001b[39mmse_loss(tract_data, x_hat, reduction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reconstruction_loss' is not defined"
     ]
    }
   ],
   "source": [
    "dropout_values = [0, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
    "\n",
    "train_rmse_results = {}\n",
    "val_rmse_results = {}\n",
    "best_val_results = {}\n",
    "\n",
    "for dropout in dropout_values:\n",
    "    print(f\"\\nTraining with dropout = {dropout}\")\n",
    "    vae_one_tract = Conv1DAutoencoder_one_tract(latent_dims=20, dropout=dropout).to(device)\n",
    "    train_rmse, val_rmse, best_val_loss = train_first_tract_dropout_experiment(\n",
    "        vae_one_tract, train_loader, val_loader, epochs=100, lr=1e-3, num_selected_tracts=8\n",
    "    )\n",
    "    train_rmse_results[dropout] = train_rmse\n",
    "    val_rmse_results[dropout] = val_rmse\n",
    "    best_val_results[dropout] = best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot best validation RMSE for each dropout value\n",
    "plt.figure(figsize=(10, 6))\n",
    "dropout_keys = sorted(best_val_results.keys())\n",
    "best_vals = [best_val_results[d] for d in dropout_keys]\n",
    "plt.plot(dropout_keys, best_vals, marker='o')\n",
    "plt.xlabel(\"Dropout\")\n",
    "plt.ylabel(\"Best Validation RMSE\")\n",
    "plt.title(\"Best Validation RMSE vs. Dropout\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Training RMSE only\n",
    "plt.figure(figsize=(18, 6))\n",
    "for dropout in dropout_values:\n",
    "    plt.plot(\n",
    "        range(1, len(train_rmse_results[dropout]) + 1),\n",
    "        train_rmse_results[dropout],\n",
    "        label=f\"Train RMSE (Dropout = {dropout})\",\n",
    "    )\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Training RMSE for Different Dropout Values\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Validation RMSE only\n",
    "plt.figure(figsize=(18, 6))\n",
    "for dropout in dropout_values:\n",
    "    plt.plot(\n",
    "        range(1, len(val_rmse_results[dropout]) + 1),\n",
    "        val_rmse_results[dropout],\n",
    "        label=f\"Val RMSE (Dropout = {dropout})\",\n",
    "    )\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Validation RMSE for Different Dropout Values\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Both Training and Validation RMSE\n",
    "plt.figure(figsize=(18, 8))\n",
    "for dropout in dropout_values:\n",
    "    plt.plot(\n",
    "        range(1, len(train_rmse_results[dropout]) + 1),\n",
    "        train_rmse_results[dropout],\n",
    "        label=f\"Train RMSE (Dropout = {dropout})\",\n",
    "    )\n",
    "    plt.plot(\n",
    "        range(1, len(val_rmse_results[dropout]) + 1),\n",
    "        val_rmse_results[dropout],\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Val RMSE (Dropout = {dropout})\",\n",
    "    )\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Train vs. Validation RMSE for Different Dropout Values\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afq_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
